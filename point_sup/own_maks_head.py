# Author: leezeeyee
# Date: 2021/10/26
import itertools

import numpy as np
import torch
import torch.nn.functional as F
from mmcv.ops import point_sample
from mmcv.runner import force_fp32

from mmdet.core import bbox2roi
from mmdet.models.roi_heads import StandardRoIHead
from mmdet.models.roi_heads.mask_heads import FCNMaskHead
from mmdet.models.builder import HEADS


from mmdet.core import (
    bbox2roi,
    bbox_mapping_back,
)


@HEADS.register_module()
class MyPointSupRoIHead(StandardRoIHead):
    """Simplest base roi head including one bbox head and one mask head."""

    def __init__(
        self,
        bbox_roi_extractor=None,
        bbox_head=None,
        mask_roi_extractor=None,
        mask_head=None,
        shared_head=None,
        train_cfg=None,
        test_cfg=None,
        pretrained=None,
        init_cfg=None,
    ):
        super(StandardRoIHead, self).__init__(
            bbox_roi_extractor=bbox_roi_extractor,
            bbox_head=bbox_head,
            mask_roi_extractor=mask_roi_extractor,
            mask_head=mask_head,
            shared_head=shared_head,
            train_cfg=train_cfg,
            test_cfg=test_cfg,
            pretrained=pretrained,
            init_cfg=init_cfg,
        )

        self.init_assigner_sampler()
        self.init_point_sample_table()

    def init_point_sample_table(self):
        try:
            self.Point_N = self.train_cfg["Point_N"]
        except:
            self.Point_N = self.test_cfg["Point_N"]

        self.point_dict = self.create_point_dict(self.Point_N)

    def create_point_dict(self, N):
        sample = N // 2
        possible_table = np.array([i for i in itertools.combinations(range(N), sample)])
        point_dict = dict(possible_table=possible_table, Point_N=N, sample=sample)
        return point_dict

    def forward_train(
        self,
        x,
        img_metas,
        proposal_list,
        gt_bboxes,
        gt_labels,
        gt_bboxes_ignore=None,
        my_sites_imgs=None,
        points_labels=None,
        **kwargs
    ):
        """
        Args:
            x (list[Tensor]): list of multi-level img features.
            img_metas (list[dict]): list of image info dict where each dict
                has: 'img_shape', 'scale_factor', 'flip', and may also contain
                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.
                For details on the values of these keys see
                `mmdet/datasets/pipelines/formatting.py:Collect`.
            proposals (list[Tensors]): list of region proposals.
            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with
                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
            gt_labels (list[Tensor]): class indices corresponding to each box
            gt_bboxes_ignore (None | list[Tensor]): specify which bounding
                boxes can be ignored when computing the loss.
            gt_masks (None | Tensor) : true segmentation masks for each box
                used if the architecture supports a segmentation task.

        Returns:
            dict[str, Tensor]: a dictionary of loss components
        """
        # assign gts and sample proposals
        if self.with_bbox or self.with_mask:
            num_imgs = len(img_metas)
            if gt_bboxes_ignore is None:
                gt_bboxes_ignore = [None for _ in range(num_imgs)]
            sampling_results = []
            for i in range(num_imgs):
                assign_result = self.bbox_assigner.assign(
                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i], gt_labels[i]
                )
                sampling_result = self.bbox_sampler.sample(
                    assign_result,
                    proposal_list[i],
                    gt_bboxes[i],
                    gt_labels[i],
                    feats=[lvl_feat[i][None] for lvl_feat in x],
                )
                sampling_results.append(sampling_result)

        losses = dict()
        # bbox head forward and loss
        if self.with_bbox:
            bbox_results = self._bbox_forward_train(
                x, sampling_results, gt_bboxes, gt_labels, img_metas
            )
            losses.update(bbox_results["loss_bbox"])

        # mask head forward and loss
        if self.with_mask:
            mask_results = self._mask_forward_train(
                x,
                sampling_results,
                bbox_results["bbox_feats"],
                sites_img=my_sites_imgs,
                points_labels=points_labels,
                gt_bboxes=gt_bboxes,
                **kwargs
            )
            losses.update(mask_results["loss_mask"])

        return losses

    def my_unmold_mask(self, mask, bbox, image_shape):
        """Converts a mask generated by the neural network to a format similar
        to its original shape.
        mask: [height, width] of type float. A small, typically 28x28 mask.
        bbox: [y1, x1, y2, x2]. The box to fit the mask in.
        Returns a binary mask with the same size as the original image."""
        # threshold = 0.5
        try:
            x1, y1, x2, y2 = bbox.detach().cpu().numpy().astype(np.int)
            new_mask = F.interpolate(
                mask.unsqueeze(0), (y2 - y1, x2 - x1), mode="bilinear"
            )
            # mask = mmcv.imresize(mask, (y2 - y1, x2 - x1))
            # mask = np.where(mask >= threshold, 1, 0).astype(np.bool)
            # Put the mask in the right location.
            full_mask = torch.zeros((80,) + image_shape[:2])
            if y2 > image_shape[:2][0]:
                new_mask[0] = new_mask[0][: image_shape[:2][0] - y2]
                y2 = image_shape[:2][0]
            if x2 > image_shape[:2][1]:
                new_mask[0] = new_mask[0][:, : image_shape[:2][1] - x2]
                x2 = image_shape[:2][1]

            full_mask[:, y1:y2, x1:x2] = new_mask[0]
        except Exception as e:
            print("in my_unmold_mask ", e)

        return full_mask

    def my_unmold_points(self, mask, bbox, image_shape, site, label):
        """Converts a mask generated by the neural network to a format similar
        to its original shape.
        mask: [height, width] of type float. A small, typically 28x28 mask.
        bbox: [y1, x1, y2, x2]. The box to fit the mask in.
        Returns a binary mask with the same size as the original image."""
        try:
            # sample site for N/2 points
            N = site.shape[1]
            samples = N // 2
            sample_index = np.random.choice(N, samples, replace=False)
            sample_site = site[:, sample_index]
            sample_label = label.unsqueeze(0)[:, sample_index]
            full_mask = self.my_unmold_mask(mask, bbox, image_shape)
            pred_sample_label = full_mask[
                :, sample_site[1].to(torch.long), sample_site[0].to(torch.long)
            ]
            return pred_sample_label, sample_label
        except Exception as e:
            print("error in my_unmold_points ", e)
            breakpoint()

    def mapping_back_bboxes(self, sampling_results, img_metas):

        mapping_back_bboxes = []
        for img_meta, res in zip(img_metas, sampling_results):
            img_shape = img_meta["img_shape"]
            scale_factor = img_meta["scale_factor"]
            flip = img_meta["flip"]
            mapping_back_bbox = bbox_mapping_back(
                res.pos_bboxes, img_shape, scale_factor, flip
            )
            mapping_back_bboxes.append(mapping_back_bbox)

        back_bboxes = torch.cat(mapping_back_bboxes)

        return back_bboxes

    def mapping_back_point_optim(self, masks, sites, back_bboxes):
        """map value of masks on given sites with the bboxes of the masks

        Args:
            masks (torch.tensor): prediction mask of the mask head `(N, C, H, W)`
            sites (torch.tensor): sample site of the labeled dataset `(N, 2, Point_N)`
            back_bboxes (torch.tensor): the bbox of the masks on origin image `(N, 4)`

        Returns:
            torch.tensor: the value of given sites `(N, C, Point_N)`
        """
        try:
            masks_trans = masks.permute(0, 2, 3, 1)  # N, H, W, C

            w_scale = (sites[:, 0] - back_bboxes[:, 0].unsqueeze(1)) / (
                back_bboxes[:, 2] - back_bboxes[:, 0]
            ).unsqueeze(1)
            h_scale = (sites[:, 1] - back_bboxes[:, 1].unsqueeze(1)) / (
                back_bboxes[:, 3] - back_bboxes[:, 1]
            ).unsqueeze(1)

            m_x = w_scale * 28
            m_y = h_scale * 28

            # limit
            m_x[m_x < 0] = 0.1
            m_y[m_y < 0] = 0.1

            m_x[m_x > 27] = 26.9
            m_y[m_y > 27] = 26.9

            m_x1 = m_x.floor().to(torch.long)
            m_x2 = m_x.ceil().to(torch.long)
            m_y1 = m_y.floor().to(torch.long)
            m_y2 = m_y.ceil().to(torch.long)

            w_l = m_x - m_x1
            w_r = 1 - w_l
            h_u = m_y - m_y1
            h_d = 1 - h_u

            point_preds = []

            N, Point_N = m_y1.shape
            first_dim = torch.arange(N).reshape(N, 1).repeat(1, Point_N).flatten()

            m_y1s = m_y1.flatten()
            m_x1s = m_x1.flatten()
            m_y2s = m_y2.flatten()
            m_x2s = m_x2.flatten()

            x1_y1s = masks_trans[first_dim, m_y1s, m_x1s]
            x1_y2s = masks_trans[first_dim, m_y2s, m_x1s]
            x2_y1s = masks_trans[first_dim, m_y1s, m_x2s]
            x2_y2s = masks_trans[first_dim, m_y2s, m_x2s]

            x1_y1s = x1_y1s.reshape(N, Point_N, -1)
            x1_y2s = x1_y2s.reshape(N, Point_N, -1)
            x2_y1s = x2_y1s.reshape(N, Point_N, -1)
            x2_y2s = x2_y2s.reshape(N, Point_N, -1)

            m_w_u = w_l[:, :, None] * x1_y1s + w_r[:, :, None] * x2_y1s
            m_w_d = w_l[:, :, None] * x1_y2s + w_r[:, :, None] * x2_y2s
            m_all = h_u[:, :, None] * m_w_u + h_d[:, :, None] * m_w_d

            point_preds = m_all.permute(0, 2, 1)  # N, C, P
            return point_preds
        except Exception as e:
            print("error in mapping_back_point_optim:", e)

    def mapping_back_point(self, sampling_results, img_metas, sites, masks):

        back_bboxes = self.mapping_back_bboxes(sampling_results, img_metas)
        point_preds = self.mapping_back_point_optim(masks, sites, back_bboxes)

        return point_preds

    def _mask_forward_train_old(
        self,
        x,
        sampling_results,
        bbox_feats,
        img_metas,
        points_labels,
        sites_img,
        **kwargs
    ):
        """Run forward function and calculate loss for mask head in
        training."""
        if not self.share_roi_extractor:
            pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])
            mask_results = self._mask_forward(x, pos_rois)
        else:
            pos_inds = []
            device = bbox_feats.device
            for res in sampling_results:
                pos_inds.append(
                    torch.ones(
                        res.pos_bboxes.shape[0], device=device, dtype=torch.uint8
                    )
                )
                pos_inds.append(
                    torch.zeros(
                        res.neg_bboxes.shape[0], device=device, dtype=torch.uint8
                    )
                )
            pos_inds = torch.cat(pos_inds)

            mask_results = self._mask_forward(
                x, pos_inds=pos_inds, bbox_feats=bbox_feats
            )

        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])
        sites = torch.cat(sites_img)
        mask_targets = torch.cat(points_labels)
        # sample
        rand_sites = []
        rand_targets = []

        Point_N = self.Point_N
        possible_table = self.point_dict["possible_table"]

        try:
            for sites, mask_targets in zip(sites_img, points_labels):
                N = len(sites)
                first_dim = (
                    torch.arange(N).reshape(N, 1).repeat(1, Point_N // 2).flatten()
                )
                sample_indexes = np.random.choice(len(possible_table), N)
                sample_table = possible_table[sample_indexes].flatten()
                each_rand_sites = (
                    sites[first_dim, :, sample_table].reshape(N, -1, 2).permute(0, 2, 1)
                )
                each_rand_targets = mask_targets[first_dim, sample_table].reshape(N, -1)
                rand_sites.append(each_rand_sites)
                rand_targets.append(each_rand_targets)
            sites = torch.cat(
                [
                    site_img[res.pos_assigned_gt_inds, :, :]
                    for site_img, res in zip(rand_sites, sampling_results)
                ]
            )
            mask_targets = torch.cat(
                [
                    labels[
                        res.pos_assigned_gt_inds,
                    ]
                    for labels, res in zip(rand_targets, sampling_results)
                ]
            )

            point_preds = self.mapping_back_point(
                sampling_results, img_metas, sites, mask_results["mask_pred"]
            )
            # 在这里找到mask对应的位置
            loss_mask = self.mask_head.loss(
                point_preds, mask_targets.to(torch.float32).squeeze(1), pos_labels
            )
            mask_results.update(loss_mask=loss_mask)  # , mask_targets=mask_targets)
            return mask_results
        except Exception as e:
            print("error in _mask_forward_train: ", e)

    def my_map_mask(self, feats, img_metas, det_bboxes, det_labels, res):
        """Test for mask head with test time augmentation."""
        # det_bboxes=det_bboxes[0]
        # det_labels=det_labels[:len(det_bboxes)]
        if det_bboxes.shape[0] == 0:
            segm_result = [[0] * 10 for _ in range(self.mask_head.num_classes)]
        else:
            aug_masks = []
            # for x, img_meta in zip(feats, img_metas):
            try:
                x = feats
                #
                img_meta = img_metas
                img_shape = img_meta["img_shape"]
                scale_factor = img_meta["scale_factor"]
                flip = img_meta["flip"]
                flip_direction = img_meta["flip_direction"]

                pos_rois = bbox2roi([det_bboxes])
                mask_results = self._mask_forward(feats, pos_rois)

                aug_masks.append(
                    mask_results["mask_pred"].sigmoid().detach().cpu().numpy()
                )
            except Exception as e:
                print(e)
                breakpoint()
            # breakpoint()
            # merged_masks = merge_aug_masks(aug_masks, [[img_metas]], self.test_cfg)

            # ori_shape = img_metas['ori_shape']
            ori_shape = img_metas["img_shape"]
            scale_factor = det_bboxes.new_ones(4)
            mask_pred = mask_results["mask_pred"]
            segm_result = self.mask_head.my_get_allign_masks(
                mask_pred,
                det_bboxes,
                det_labels,
                self.test_cfg,
                ori_shape,
                scale_factor=scale_factor,
                rescale=False,
            )
            # segm_result = self.mask_head.get_seg_masks(merged_masks,det_bboxes,det_labels,self.test_cfg,ori_shape,scale_factor=scale_factor,rescale=False)
        return segm_result

    def _mask_forward(self, x, rois=None, pos_inds=None, bbox_feats=None):
        """Mask head forward function used in both training and testing."""
        assert (rois is not None) ^ (pos_inds is not None and bbox_feats is not None)
        if rois is not None:
            mask_feats = self.mask_roi_extractor(
                x[: self.mask_roi_extractor.num_inputs], rois
            )
            if self.with_shared_head:
                mask_feats = self.shared_head(mask_feats)
        else:
            assert bbox_feats is not None
            mask_feats = bbox_feats[pos_inds]

        mask_pred = self.mask_head(mask_feats)
        mask_results = dict(mask_pred=mask_pred, mask_feats=mask_feats)
        return mask_results

    def _mask_forward_train(
        self, x, sampling_results, bbox_feats, points_labels, sites_img, **kwargs
    ):
        """Run forward function and calculate loss for mask head in
        training."""
        if not self.share_roi_extractor:
            pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])
            mask_results = self._mask_forward(x, pos_rois)
        else:
            pos_inds = []
            device = bbox_feats.device
            for res in sampling_results:
                pos_inds.append(
                    torch.ones(
                        res.pos_bboxes.shape[0], device=device, dtype=torch.uint8
                    )
                )
                pos_inds.append(
                    torch.zeros(
                        res.neg_bboxes.shape[0], device=device, dtype=torch.uint8
                    )
                )
            pos_inds = torch.cat(pos_inds)

            mask_results = self._mask_forward(
                x, pos_inds=pos_inds, bbox_feats=bbox_feats
            )
        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])
        rand_sites = []
        rand_targets = []

        Point_N = self.Point_N
        possible_table = self.point_dict["possible_table"]

        try:
            for sites, mask_targets in zip(sites_img, points_labels):
                N = len(sites)
                first_dim = (
                    torch.arange(N).reshape(N, 1).repeat(1, Point_N // 2).flatten()
                )
                sample_indexes = np.random.choice(len(possible_table), N)
                sample_table = possible_table[sample_indexes].flatten()
                each_rand_sites = (
                    sites[first_dim, :, sample_table].reshape(N, -1, 2).permute(0, 2, 1)
                )
                each_rand_targets = mask_targets[first_dim, sample_table].reshape(N, -1)

                rand_sites.append(each_rand_sites)
                rand_targets.append(each_rand_targets)

            sites = torch.cat(
                [
                    site_img[res.pos_assigned_gt_inds, :, :]
                    for site_img, res in zip(rand_sites, sampling_results)
                ]
            )
            mask_targets = torch.cat(
                [
                    labels[
                        res.pos_assigned_gt_inds,
                    ]
                    for labels, res in zip(rand_targets, sampling_results)
                ]
            )
            pos_bboxes = torch.cat([res.pos_bboxes for res in sampling_results])
            new_sites = get_point_coords_wrt_box(pos_bboxes, sites)
            point_ignores = (
                (new_sites[:, :, 0] < 0)
                | (new_sites[:, :, 0] > 1)
                | (new_sites[:, :, 1] < 0)
                | (new_sites[:, :, 1] > 1)
            )
            mask_targets[point_ignores] = 2

            point_preds = point_sample(
                mask_results["mask_pred"],
                new_sites,
                align_corners=False,
            )
            loss_mask = self.mask_head.loss(
                point_preds, mask_targets.to(torch.float32).squeeze(1), pos_labels
            )
            mask_results.update(loss_mask=loss_mask)
            return mask_results
        except Exception as e:
            print("error in _mask_forward_train: ", e)


def get_point_coords_wrt_box(boxes_coords, point_coords):
    """
    Convert image-level absolute coordinates to box-normalized [0, 1] x [0, 1] point cooordinates.
    Args:
        boxes_coords (Tensor): A tensor of shape (R, 4) that contains bounding boxes.
            coordinates.
        point_coords (Tensor): A tensor of shape (R, P, 2) that contains
            image-normalized coordinates of P sampled points.
    Returns:
        point_coords_wrt_box (Tensor): A tensor of shape (R, P, 2) that contains
            [0, 1] x [0, 1] box-normalized coordinates of the P sampled points.
    """
    with torch.no_grad():
        point_coords_wrt_box = point_coords.clone().permute(0, 2, 1).to(torch.float32)
        point_coords_wrt_box[:, :, 0] -= boxes_coords[:, None, 0]
        point_coords_wrt_box[:, :, 1] -= boxes_coords[:, None, 1]
        point_coords_wrt_box[:, :, 0] = point_coords_wrt_box[:, :, 0] / (
            boxes_coords[:, None, 2] - boxes_coords[:, None, 0]
        )
        point_coords_wrt_box[:, :, 1] = point_coords_wrt_box[:, :, 1] / (
            boxes_coords[:, None, 3] - boxes_coords[:, None, 1]
        )
    return point_coords_wrt_box
